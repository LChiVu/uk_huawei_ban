# -*- coding: utf-8 -*-
"""da401 huawei analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19IBsP4tjxaamxndF3u2lvw7xMzUdx1vk

# 1- Reading and preprocessing the dataset
"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd

# Download necessary NLTK data for stopwords and lemmatization
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Load primary data Excel file
new_excel_path = '/content/huawei articles Local.xlsx'
df = pd.read_excel(new_excel_path)

def preprocess_text(text, remove_urls=True):
    """
    Preprocesses the given text by applying a series of cleaning steps.

    Parameters:
    text: The text to preprocess.
    remove_urls: A boolean indicating whether URLs should be removed from the text.

    Return: The cleaned text.
    """

    # Remove URLs
    if remove_urls:
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Lowercasing
    text = text.lower()

    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))

    text = ' '.join([word for word in text.split() if word not in stop_words])

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])

    return text

# Apply the comprehensive text preprocessing function to the 'Article' column
df['Cleaned_Article'] = df['Article'].apply(lambda x: preprocess_text(x,
                                                                        remove_urls=True,
                                                                        ))

# Export CSV file with cleaned data (1st clean) for exploratory visualizations
df.to_csv('/content/Cleaned_Huawei.csv', index=False)

# Display first few rows of dataframe
df[['Source','Article', 'Cleaned_Article']].head()

"""
#Removing all stopwords (words that do not provide context for analysis)
"""

# Define additional stopwords for this specific dataset
additional_stopwords = ['Huawei', '5G', 'UK','China',
                        'huawei', '5g', 'uk','china',
                        'chinese','britain','edt', 'time',
                        'email','forbes','chinauk', 'mr', 'july', 'british',
                        'country','fox','fox news','reuters', 'scroll',
                        'g','uk ','u','news','new','united', 'fake', 'hong', 'kong', 'hong kong'
                        'british', ' uk','said','state','skip advertisement','advertisement skip',
                        'term service','advertisement scroll','ban','year','advertisement','government',
                        'article year','year old','social medium','mate pro','scroll continue','telecom',
                        'network','company','phone','huaweis','gt', 'getty', 'image',
                        'article','channel','ofcom','cgtn', 'decision', 'term', 'service','getty image',
                        'read','mahbubani','zhao lijian','th','report ad','nm','scroll continue','mate','ad','fake account','equipment','prohuawei','article','skip','bst'
                        ]


def remove_verified_stopwords(df, stopwords, column='Cleaned_Article'):
    """
    Removes verified stopwords from the specified DataFrame column and returns a cleaned DataFrame.

    Parameters:
    df: The dataframe containing the processed text.
    stopwords: A list of stopwords to check for and remove from the text.
    column: The name of the DataFrame column to clean.

    Return: A datafram with the specified stopwords removed from the given column.
    """
    # Function to remove stopwords from a single piece of text
    def remove_stopwords_from_text(text, stopwords):
        tokens = text.split()
        cleaned_tokens = [token for token in tokens if token not in stopwords]
        return ' '.join(cleaned_tokens)

    # Run function on each row in the DataFrame
    df[column] = df[column].apply(lambda x: remove_stopwords_from_text(x, stopwords))

    return df

df = remove_verified_stopwords(df, additional_stopwords)
print(df.head())

# Mapping for normalizing and merging sources
source_normalization_mapping = {
    'Sina (from 中国新闻网）': 'Sina',
    'Sina (from 海外网）': 'Sina',
    'Sina (from 环球网）': 'Sina',
    'Guardian': 'The Guardian',
    'Guardian ': 'The Guardian',  # In case of trailing space
}

# Apply the mapping to normalize source names
df['Source'] = df['Source'].map(lambda x: source_normalization_mapping.get(x, x))

# Now, your DataFrame 'df' should have the 'Source' column normalized as per the mapping.

# Remove rows where 'Source' is NaN
df = df.dropna(subset=['Source'])

df.Source.unique()

# Group by 'Source' and concatenate all articles for each source
df = df.groupby('Source')['Cleaned_Article'].apply(lambda x: ' '.join(x)).reset_index()

# 'df' now has two columns: 'Source' and 'Cleaned_Article'
# 'Cleaned_Article' contains all content of articles from each source in 1 string

# Now we can apply TF-IDF on the 'Cleaned_Article'

df

"""# 2- TF-IDF Keyword Analysis"""

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Initializing
tfidf_vectorizer = TfidfVectorizer(max_features=10000,
                                   ngram_range=(1, 2),    # both single and bi-grams
                                   stop_words='english')  # Use English stop words

# Fit and transform the 'Cleaned_Article' column to create a vectorized form of the text data
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Cleaned_Article'])
feature_names = np.array(tfidf_vectorizer.get_feature_names_out())

# Function to get top n tfidf values in row and return them with their corresponding feature names
def top_n_words(row_data, features, n):
    top_n_idx = np.argsort(row_data)[::-1][:n]
    top_n_values = row_data[top_n_idx]
    return features[top_n_idx], top_n_values

# Number of top words
top_n = 20

# Iterate through each source to find and print the top 20 words based on TF-IDF scores
for i, source in enumerate(df['Source']):
    row_data = tfidf_matrix[i].toarray().flatten()
    top_words, top_scores = top_n_words(row_data, feature_names, top_n)
    print(f"Source: {source}")
    for word, score in zip(top_words, top_scores):
        print(f"{word}: {score:.4f}")
    print("\n" + "-"*50 + "\n")

"""##2.1- Ploting Keywords For Each News Source"""

import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# Function to generate plots for each source
def plot_sources_tfidf(tfidf_matrix, feature_names, sources, top_n=20, max_plots=None):
    # Determine the number of sources to plot
    num_sources = len(sources) if max_plots is None else min(len(sources), max_plots)

    for i in range(num_sources):
        # Get TF-IDF scores for the current source
        source_tfidf_scores = tfidf_matrix[i].toarray().flatten()

        sorted_indices = np.argsort(source_tfidf_scores)[::-1][:top_n]
        sorted_scores = source_tfidf_scores[sorted_indices]
        sorted_words = feature_names[sorted_indices]

        # Plot
        plt.figure(figsize=(10, 8))
        plt.barh(range(top_n), sorted_scores[::-1], color='skyblue')
        plt.yticks(range(top_n), sorted_words[::-1])
        plt.xlabel('TF-IDF Score')
        plt.title(f'Top {top_n} words for source: {sources[i]}')
        plt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top
        plt.show()

# Get the source names
source_names = df['Source'].tolist()

plot_sources_tfidf(tfidf_matrix, feature_names, source_names, top_n=20, max_plots=5)

"""##2.2- Plotting Bigrams For Each News Source"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Adjust TF-IDF Vectorizer to focus on bigrams only
tfidf_vectorizer_bigrams = TfidfVectorizer(ngram_range=(2, 2),  # Focus on bigrams
                                           max_features=10000,
                                           stop_words='english')

# vectorized form of the text data for bigrams
tfidf_matrix_bigrams = tfidf_vectorizer_bigrams.fit_transform(df['Cleaned_Article'])

# Feature names (bigrams) in the TF-IDF matrix
feature_names_bigrams = np.array(tfidf_vectorizer_bigrams.get_feature_names_out())

# Modified plot function for bigrams
def plot_sources_bigrams(tfidf_matrix, feature_names, sources, top_n=20, max_plots=None):
    num_sources = len(sources) if max_plots is None else min(len(sources), max_plots)

    for i in range(num_sources):
        # Get TF-IDF scores for the current source
        source_tfidf_scores = tfidf_matrix[i].toarray().flatten()

        # Sort scores and select top n
        sorted_indices = np.argsort(source_tfidf_scores)[::-1][:top_n]
        sorted_scores = source_tfidf_scores[sorted_indices]
        sorted_bigrams = feature_names[sorted_indices]

        # Plot
        plt.figure(figsize=(10, 8))
        plt.barh(range(top_n), sorted_scores[::-1], color='lightcoral')
        plt.yticks(range(top_n), sorted_bigrams[::-1], fontsize=10)
        plt.xlabel('TF-IDF Score')
        plt.title(f'Top {top_n} bigrams for source: {sources[i]}')
        plt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top
        plt.show()


source_names = df['Source'].tolist()

plot_sources_bigrams(tfidf_matrix_bigrams, feature_names_bigrams, source_names, top_n=20, max_plots=5)

"""#3- Mapping of Sources to Countries"""

# Define the mapping from source to country
source_to_country = {
    'Daily Mail': 'UK',
    'Global Times': 'China',
    'The Guardian': 'UK',
    'NBC News': 'US',
    'Reuters': 'UK',  # Reuters operates globally, but focusing on the UK reporting for this project
    'Sina': 'China',
    'Sohu': 'China',
    '人民日报': 'China',  # People's Daily
    'Forbes': 'US',
    'Fox News': 'US',
    'NYT': 'US'  # NYT stands for The New York Times
}

# Apply the mapping to the 'Source' column to create a new 'Country' column
df['Country'] = df['Source'].map(source_to_country)

df.head()

"""##3.1- Ploting Keywords For Each Country"""

# Group the DataFrame by 'Country'
grouped = df.groupby('Country')

country_keywords = {}

for country, group in grouped:
    # Apply TF-IDF to the 'Cleaned_Article' text of each country's segment
    tfidf_matrix = tfidf_vectorizer.fit_transform(group['Cleaned_Article'])


    feature_names = tfidf_vectorizer.get_feature_names_out()
    avg_tfidf_scores = tfidf_matrix.mean(axis=0)

    # Convert to a readable format and sort
    scores_dict = {feature_names[i]: avg_tfidf_scores[0, i] for i in range(len(feature_names))}
    sorted_scores = sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)

    # Store the top 10 keywords for the country
    country_keywords[country] = sorted_scores[:10]

# Function to plot the top keywords for each country
def plot_top_keywords(country_keywords):
    n_countries = len(country_keywords)
    n_cols = 2
    n_rows = (n_countries + 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))
    axes = axes.flatten()

    for i, (country, keywords) in enumerate(country_keywords.items()):
        words, scores = zip(*keywords)

        axes[i].barh(words, scores, color='skyblue')
        axes[i].set_title(f'Top 10 Keywords for {country}')
        axes[i].invert_yaxis()  # Highest scores on top
        axes[i].set_xlabel('TF-IDF Score')

    # Hide any unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()

#  'country_keywords' already defined from the previous TF-IDF analysis
plot_top_keywords(country_keywords)
print(country_keywords)

country_keywords

"""##3.2- Plotting Bi-Grams For Each Country"""

# same thing as privious
grouped = df.groupby('Country')

country_bigrams = {}

for country, group in grouped:
    tfidf_matrix = tfidf_vectorizer_bigrams.fit_transform(group['Cleaned_Article'])

    feature_names = tfidf_vectorizer_bigrams.get_feature_names_out()
    avg_tfidf_scores = tfidf_matrix.mean(axis=0)

    scores_dict = {feature_names[i]: avg_tfidf_scores[0, i] for i in range(len(feature_names))}
    sorted_scores = sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)

    # Store the top 10 bigrams for the country
    country_bigrams[country] = sorted_scores[:10]

# Function to plot the top bigrams for each country
def plot_top_bigrams(country_bigrams):
    n_countries = len(country_bigrams)
    n_cols = 2
    n_rows = (n_countries + 1) // n_cols
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))
    axes = axes.flatten()

    for i, (country, bigrams) in enumerate(country_bigrams.items()):
        bigram_texts, scores = zip(*bigrams)

        axes[i].barh(bigram_texts, scores, color='lightgreen')
        axes[i].set_title(f'Top 10 Bigrams for {country}')
        axes[i].invert_yaxis()
        axes[i].set_xlabel('TF-IDF Score')

    # Hide any unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()


plot_top_bigrams(country_bigrams)

country_bigrams

"""#4- Sentiment Analysis For Each Source"""

from textblob import TextBlob
import numpy as np

# Calculate sentiment polarity for each article
df['TextBlob_Sentiment'] = df['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Aggregate the sentiment scores by source
source_sentiment = df.groupby('Source')['TextBlob_Sentiment'].mean().reset_index()
import matplotlib.pyplot as plt

# Sort the sources
source_sentiment_sorted = source_sentiment.sort_values(by='TextBlob_Sentiment')

# Plotting
plt.figure(figsize=(10, 8))
plt.barh(source_sentiment_sorted['Source'], source_sentiment_sorted['TextBlob_Sentiment'], color='skyblue')
plt.xlabel('Average TextBlob Polarity Score')
plt.title('Average TextBlob Polarity Score by Source')
plt.show()

from textblob import TextBlob
import numpy as np

# Calculate sentiment subjectivity for each article
df['TextBlob_Sentiment'] = df['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

source_sentiment = df.groupby('Source')['TextBlob_Sentiment'].mean().reset_index()
import matplotlib.pyplot as plt

# Sort the sources
source_sentiment_sorted = source_sentiment.sort_values(by='TextBlob_Sentiment')

# Plotting
plt.figure(figsize=(10, 8))
plt.barh(source_sentiment_sorted['Source'], source_sentiment_sorted['TextBlob_Sentiment'], color='blue')
plt.xlabel('Average TextBlob Subjectivity Score')
plt.title('Average TextBlob Subjectivity Score by Source')
plt.show()

"""#5- Checking sentiment by country"""

# Combine all articles within each country into a single text
merged_texts_by_country = df.groupby('Country')['Cleaned_Article'].apply(' '.join).reset_index()

# Calculate polarity sentiment for each merged country text
merged_texts_by_country['Polarity'] = merged_texts_by_country['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Plot
plt.figure(figsize=(10, 6))
plt.bar(merged_texts_by_country['Country'], merged_texts_by_country['Polarity'], color=['blue', 'red', 'green'])
plt.xlabel('Country')
plt.ylabel('Average Sentiment Score (polarity)')
plt.title('Overall Sentiment Score by Country (polarity)')
plt.xticks(rotation=45)
plt.show()

# Calculate subjectivity sentiment for each merged country text
merged_texts_by_country['Subjectivity'] = merged_texts_by_country['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

# Plot
plt.figure(figsize=(10, 6))
plt.bar(merged_texts_by_country['Country'], merged_texts_by_country['Subjectivity'], color=['blue', 'red', 'green'])
plt.xlabel('Country')
plt.ylabel('Average Sentiment Score (subjectivity)')
plt.title('Overall Sentiment Score by Country (subjectivity)')
plt.xticks(rotation=45)
plt.show()