# -*- coding: utf-8 -*-
"""REVISED da401 huawei analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19IBsP4tjxaamxndF3u2lvw7xMzUdx1vk

# 1- Reading and preprocessing the dataset
"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd

# Download necessary NLTK data for stopwords and lemmatization
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Load primary data Excel file
new_excel_path = '/content/huawei articles Local.xlsx'
df = pd.read_excel(new_excel_path)

def preprocess_text(text, remove_urls=True):
    """
    Preprocesses the given text by applying a series of cleaning steps.

    Parameters:
    text: The text to preprocess.
    remove_urls: A boolean indicating whether URLs should be removed from the text.

    Return: The cleaned text.
    """

    # Remove URLs
    if remove_urls:
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Lowercasing
    text = text.lower()

    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))

    text = ' '.join([word for word in text.split() if word not in stop_words])

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])

    return text

# Apply the comprehensive text preprocessing function to the 'Article' column
df['Cleaned_Article'] = df['Article'].apply(lambda x: preprocess_text(x,
                                                                        remove_urls=True,
                                                                        ))

# Export CSV file with cleaned data (1st clean) for exploratory visualizations
df.to_csv('/content/Cleaned_Huawei.csv', index=False)

# Display first few rows of dataframe
df[['Source','Article', 'Cleaned_Article']].head()

"""
#Removing all stopwords (words that do not provide context for analysis)
"""

# Define additional stopwords for this specific dataset
additional_stopwords = ['Huawei', '5G', 'UK','China',
                        'huawei', '5g', 'uk','china',
                        'chinese','britain','edt', 'time',
                        'email','forbes','chinauk', 'mr', 'july', 'british',
                        'country','fox','fox news','reuters', 'scroll',
                        'g','uk ','u','news','new','united', 'fake', 'hong', 'kong', 'hong kong'
                        'british', ' uk','said','state','skip advertisement','advertisement skip',
                        'term service','advertisement scroll','ban','year','advertisement','government',
                        'article year','year old','social medium','mate pro','scroll continue','telecom',
                        'network','company','phone','huaweis','gt', 'getty', 'image',
                        'article','channel','ofcom','cgtn', 'decision', 'term', 'service','getty image',
                        'read','mahbubani','zhao lijian','th','report ad','nm','scroll continue','mate','ad','fake account','equipment','prohuawei','article','skip','bst'
                        ]


def remove_verified_stopwords(df, stopwords, column='Cleaned_Article'):
    """
    Removes verified stopwords from the specified DataFrame column and returns a cleaned DataFrame.

    Parameters:
    df: The dataframe containing the processed text.
    stopwords: A list of stopwords to check for and remove from the text.
    column: The name of the DataFrame column to clean.

    Return: A datafram with the specified stopwords removed from the given column.
    """
    # Function to remove stopwords from a single piece of text
    def remove_stopwords_from_text(text, stopwords):
        tokens = text.split()
        cleaned_tokens = [token for token in tokens if token not in stopwords]
        return ' '.join(cleaned_tokens)

    # Run function on each row in the DataFrame
    df[column] = df[column].apply(lambda x: remove_stopwords_from_text(x, stopwords))

    return df

df = remove_verified_stopwords(df, additional_stopwords)
print(df.head())

# Mapping for normalizing and merging sources
source_normalization_mapping = {
    'Sina (from 中国新闻网）': 'Sina',
    'Sina (from 海外网）': 'Sina',
    'Sina (from 环球网）': 'Sina',
    'Guardian': 'The Guardian',
    'Guardian ': 'The Guardian',  # In case of trailing space
    '人民日报' : 'People`s Daily',
    'The Daily Mirror': 'Daily Mirror'
}

# Apply the mapping to normalize source names
df['Source'] = df['Source'].map(lambda x: source_normalization_mapping.get(x, x))

# Now, your DataFrame 'df' should have the 'Source' column normalized as per the mapping.

# Remove rows where 'Source' is NaN
df = df.dropna(subset=['Source'])

df.Source.unique()

# Group by 'Source' and concatenate all articles for each source
df = df.groupby('Source')['Cleaned_Article'].apply(lambda x: ' '.join(x)).reset_index()

# 'df' now has two columns: 'Source' and 'Cleaned_Article'
# 'Cleaned_Article' contains all content of articles from each source in 1 string

# Now we can apply TF-IDF on the 'Cleaned_Article'

df

"""# 2- TF-IDF Keyword Analysis"""

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Initializing
tfidf_vectorizer = TfidfVectorizer(max_features=10000,
                                   ngram_range=(1, 2),    # both single and bi-grams
                                   stop_words='english')  # Use English stop words

# Fit and transform the 'Cleaned_Article' column to create a vectorized form of the text data
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Cleaned_Article'])
feature_names = np.array(tfidf_vectorizer.get_feature_names_out())

# Function to get top n tfidf values in row and return them with their corresponding feature names
def top_n_words(row_data, features, n):
    top_n_idx = np.argsort(row_data)[::-1][:n]
    top_n_values = row_data[top_n_idx]
    return features[top_n_idx], top_n_values

# Number of top words
top_n = 20

# Iterate through each source to find and print the top 20 words based on TF-IDF scores
for i, source in enumerate(df['Source']):
    row_data = tfidf_matrix[i].toarray().flatten()
    top_words, top_scores = top_n_words(row_data, feature_names, top_n)
    print(f"Source: {source}")
    for word, score in zip(top_words, top_scores):
        print(f"{word}: {score:.4f}")
    print("\n" + "-"*50 + "\n")

"""##2.1- Ploting Keywords For Each News Source"""

import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# Function to generate plots for each source
def plot_sources_tfidf(tfidf_matrix, feature_names, sources, top_n=20, max_plots=None):
    # Determine the number of sources to plot
    num_sources = len(sources) if max_plots is None else min(len(sources), max_plots)

    for i in range(num_sources):
        # Get TF-IDF scores for the current source
        source_tfidf_scores = tfidf_matrix[i].toarray().flatten()

        sorted_indices = np.argsort(source_tfidf_scores)[::-1][:top_n]
        sorted_scores = source_tfidf_scores[sorted_indices]
        sorted_words = feature_names[sorted_indices]

        # Plot
        plt.figure(figsize=(10, 8))
        plt.barh(range(top_n), sorted_scores[::-1], color='skyblue')
        plt.yticks(range(top_n), sorted_words[::-1])
        plt.xlabel('TF-IDF Score')
        plt.title(f'Top {top_n} words for source: {sources[i]}')
        plt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top
        plt.show()

# Get the source names
source_names = df['Source'].tolist()

plot_sources_tfidf(tfidf_matrix, feature_names, source_names, top_n=20, max_plots=5)

"""##2.2- Plotting Bigrams For Each News Source"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Adjust TF-IDF Vectorizer to focus on bigrams only
tfidf_vectorizer_bigrams = TfidfVectorizer(ngram_range=(2, 2),  # Focus on bigrams
                                           max_features=10000,
                                           stop_words='english')

# vectorized form of the text data for bigrams
tfidf_matrix_bigrams = tfidf_vectorizer_bigrams.fit_transform(df['Cleaned_Article'])

# Feature names (bigrams) in the TF-IDF matrix
feature_names_bigrams = np.array(tfidf_vectorizer_bigrams.get_feature_names_out())

# Modified plot function for bigrams
def plot_sources_bigrams(tfidf_matrix, feature_names, sources, top_n=20, max_plots=None):
    num_sources = len(sources) if max_plots is None else min(len(sources), max_plots)

    for i in range(num_sources):
        # Get TF-IDF scores for the current source
        source_tfidf_scores = tfidf_matrix[i].toarray().flatten()

        # Sort scores and select top n
        sorted_indices = np.argsort(source_tfidf_scores)[::-1][:top_n]
        sorted_scores = source_tfidf_scores[sorted_indices]
        sorted_bigrams = feature_names[sorted_indices]

        # Plot
        plt.figure(figsize=(10, 8))
        plt.barh(range(top_n), sorted_scores[::-1], color='lightcoral')
        plt.yticks(range(top_n), sorted_bigrams[::-1], fontsize=10)
        plt.xlabel('TF-IDF Score')
        plt.title(f'Top {top_n} bigrams for source: {sources[i]}')
        plt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top
        plt.show()


source_names = df['Source'].tolist()

plot_sources_bigrams(tfidf_matrix_bigrams, feature_names_bigrams, source_names, top_n=20, max_plots=5)

"""#3- Mapping of Sources to Countries"""

# Define the mapping from source to country
source_to_country = {
    'Daily Mail': 'UK',
    'Global Times': 'China',
    'The Guardian': 'UK',
    'Daily Mirror': 'UK',
    'NBC News': 'US',
    'Reuters': 'UK',  # Reuters operates globally, but focusing on the UK reporting for this project
    'Sina': 'China',
    'Sohu': 'China',
    'People`s Daily': 'China',
    'Forbes': 'US',
    'Fox News': 'US',
    'NYT': 'US'  # NYT stands for The New York Times
}

# Apply the mapping to the 'Source' column to create a new 'Country' column
df['Country'] = df['Source'].map(source_to_country)

df.head()

"""##3.1- Ploting Keywords For Each Country"""

# Group the DataFrame by 'Country'
grouped = df.groupby('Country')

country_keywords = {}

for country, group in grouped:
    # Apply TF-IDF to the 'Cleaned_Article' text of each country's segment
    tfidf_matrix = tfidf_vectorizer.fit_transform(group['Cleaned_Article'])


    feature_names = tfidf_vectorizer.get_feature_names_out()
    avg_tfidf_scores = tfidf_matrix.mean(axis=0)

    # Convert to a readable format and sort
    scores_dict = {feature_names[i]: avg_tfidf_scores[0, i] for i in range(len(feature_names))}
    sorted_scores = sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)

    # Store the top 10 keywords for the country
    country_keywords[country] = sorted_scores[:10]

def plot_top_keywords(country_keywords):
    n_countries = len(country_keywords)
    n_cols = 2
    n_rows = (n_countries + 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))
    axes = axes.flatten()

    # Collect all keywords across countries
    all_keywords = {country: set([word for word, _ in keywords]) for country, keywords in country_keywords.items()}

    # Determine the shared terms for each scenario
    shared_all = all_keywords['UK'] & all_keywords['US'] & all_keywords['China']
    shared_uk_us = (all_keywords['UK'] & all_keywords['US']) - shared_all
    shared_uk_china = (all_keywords['UK'] & all_keywords['China']) - shared_all
    shared_us_china = (all_keywords['US'] & all_keywords['China']) - shared_all

    # Legend information
    legend_elements = [
        plt.Line2D([0], [0], color='red', lw=4, label='Shared by All Countries'),
        plt.Line2D([0], [0], color='blue', lw=4, label='Shared by UK and US'),
        plt.Line2D([0], [0], color='green', lw=4, label='Shared by UK and China'),
        plt.Line2D([0], [0], color='purple', lw=4, label='Shared by US and China'),
        plt.Line2D([0], [0], color='skyblue', lw=4, label='Unique to Country')
    ]

    for i, (country, keywords) in enumerate(country_keywords.items()):
        words, scores = zip(*keywords)

        # Assign colors based on sharing status
        colors = []
        for word in words:
            if word in shared_all:
                colors.append('red')
            elif word in shared_uk_us and (country == 'UK' or country == 'US'):
                colors.append('blue')
            elif word in shared_uk_china and (country == 'UK' or country == 'China'):
                colors.append('green')
            elif word in shared_us_china and (country == 'US' or country == 'China'):
                colors.append('purple')
            else:
                colors.append('skyblue')

        axes[i].barh(words, scores, color=colors)
        axes[i].set_title(f'Top 10 Keywords for {country}')
        axes[i].invert_yaxis()  # Highest scores on top
        axes[i].set_xlabel('TF-IDF Score')

        # Increase font size for terms
        axes[i].tick_params(axis='y', labelsize=15)

    # Hide any unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    # Add legend
    fig.legend(handles=legend_elements, loc='lower right')
    plt.tight_layout(rect=[0.1, 0, 1, 1])

    plt.show()

plot_top_keywords(country_keywords)

"""##3.2- Plotting Bi-Grams For Each Country"""

# same thing as privious
grouped = df.groupby('Country')

country_bigrams = {}

for country, group in grouped:
    tfidf_matrix = tfidf_vectorizer_bigrams.fit_transform(group['Cleaned_Article'])

    feature_names = tfidf_vectorizer_bigrams.get_feature_names_out()
    avg_tfidf_scores = tfidf_matrix.mean(axis=0)

    scores_dict = {feature_names[i]: avg_tfidf_scores[0, i] for i in range(len(feature_names))}
    sorted_scores = sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)

    # Store the top 10 bigrams for the country
    country_bigrams[country] = sorted_scores[:10]

import matplotlib.pyplot as plt

def plot_top_bigrams(country_bigrams):
    n_countries = len(country_bigrams)
    n_cols = 2
    n_rows = (n_countries + 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))
    axes = axes.flatten()

    # Collect all bigrams across countries
    all_bigrams = {country: set([word for word, _ in bigrams]) for country, bigrams in country_bigrams.items()}

    # Determine the shared terms for each scenario
    shared_all = set.intersection(*all_bigrams.values())
    shared_uk_us = all_bigrams['UK'] & all_bigrams['US'] - shared_all
    shared_uk_china = all_bigrams['UK'] & all_bigrams['China'] - shared_all
    shared_us_china = all_bigrams['US'] & all_bigrams['China'] - shared_all

    # Legend information
    legend_elements = [
        plt.Line2D([0], [0], color='red', lw=4, label='Shared by All Countries'),
        plt.Line2D([0], [0], color='blue', lw=4, label='Shared by UK and US'),
        plt.Line2D([0], [0], color='green', lw=4, label='Shared by UK and China'),
        plt.Line2D([0], [0], color='purple', lw=4, label='Shared by US and China'),
        plt.Line2D([0], [0], color='lightgreen', lw=4, label='Unique to Country')
    ]

    for i, (country, bigrams) in enumerate(country_bigrams.items()):
        words, scores = zip(*bigrams)

        # Assign colors based on sharing status
        colors = []
        for word in words:
            if word in shared_all:
                colors.append('red')
            elif word in shared_uk_us and (country == 'UK' or country == 'US'):
                colors.append('blue')
            elif word in shared_uk_china and (country == 'UK' or country == 'China'):
                colors.append('green')
            elif word in shared_us_china and (country == 'US' or country == 'China'):
                colors.append('purple')
            else:
                colors.append('lightgreen')

        axes[i].barh(words, scores, color=colors)
        axes[i].set_title(f'Top 10 Bigrams for {country}')
        axes[i].invert_yaxis()  # Highest scores on top
        axes[i].set_xlabel('TF-IDF Score')

        # Increase font size for terms
        axes[i].tick_params(axis='y', labelsize=15)

    # Hide any unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    # Add legend to the left side of the plot
    fig.legend(handles=legend_elements, loc='lower right')
    plt.tight_layout(rect=[0.1, 0, 1, 1])

    plt.show()

# Assuming 'country_bigrams' is already populated from the previous TF-IDF analysis
plot_top_bigrams(country_bigrams)

country_bigrams

"""#4- Sentiment Analysis For Each Source

### Polarity sentiment for each source
"""

from textblob import TextBlob
import numpy as np

# Calculate sentiment polarity for each article
df['TextBlob_Sentiment'] = df['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Aggregate the sentiment scores by source
source_sentiment = df.groupby('Source')['TextBlob_Sentiment'].mean().reset_index()

# Sort the sources
source_sentiment_sorted = source_sentiment.sort_values(by='TextBlob_Sentiment')

# Map sources to countries and assign colors
colors = source_sentiment_sorted['Source'].map(source_to_country).replace({
    'UK': 'lightgreen',
    'China': 'salmon',
    'US': 'lightblue'
})

# Plotting
plt.figure(figsize=(10, 8))
plt.barh(source_sentiment_sorted['Source'], source_sentiment_sorted['TextBlob_Sentiment'], color=colors)
plt.xlabel('Polarity Score')
plt.title('Average TextBlob Polarity Score by Source')
from matplotlib.patches import Patch
legend_elements = [Patch(facecolor='lightblue', label='US'),
                   Patch(facecolor='salmon', label='China'),
                   Patch(facecolor='lightgreen', label='UK')]
plt.legend(handles=legend_elements, title="Countries")
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=13)
plt.show()



"""### Subjectivity sentiment for each source


"""

# Calculate sentiment subjectivity for each article
df['TextBlob_Sentiment'] = df['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

# Aggregate the sentiment scores by source
source_sentiment = df.groupby('Source')['TextBlob_Sentiment'].mean().reset_index()

# Sort the sources
source_sentiment_sorted = source_sentiment.sort_values(by='TextBlob_Sentiment')

# Map sources to countries and assign colors
colors = source_sentiment_sorted['Source'].map(source_to_country).replace({
    'UK': 'lightgreen',
    'China': 'salmon',
    'US': 'lightblue'
})

# Plotting
plt.figure(figsize=(10, 8))
plt.barh(source_sentiment_sorted['Source'], source_sentiment_sorted['TextBlob_Sentiment'], color=colors)
plt.xlabel('Average TextBlob Subjectivity Score')
plt.title('Average TextBlob Subjectivity Score by Source')
from matplotlib.patches import Patch
legend_elements = [Patch(facecolor='lightblue', label='US'),
                   Patch(facecolor='salmon', label='China'),
                   Patch(facecolor='lightgreen', label='UK')]
plt.legend(handles=legend_elements, title="Countries")
plt.tick_params(axis='y', labelsize=15)
plt.tick_params(axis='x', labelsize=13)
plt.show()

"""#5- Checking sentiment by country

### Polarity by country
"""

import matplotlib.pyplot as plt
from textblob import TextBlob

# Calculate polarity sentiment for each article
df['Polarity'] = df['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Group by country and collect all polarity scores per country
country_polarity = df.groupby('Country')['Polarity'].apply(list).reset_index()

# Prepare for plotting
countries = country_polarity['Country']
polarity_lists = country_polarity['Polarity']
colors = ['salmon', 'lightblue', 'lightgreen']

# Plotting bar chart for average polarity
plt.figure(figsize=(10, 6))
average_polarity = [sum(polarities) / len(polarities) if polarities else 0 for polarities in polarity_lists]
plt.bar(countries, average_polarity, color=colors)
plt.xlabel('Country')
plt.ylabel('Average Polarity Score')
plt.title('Overall Score by Country - Polarity')
plt.xticks(rotation=45)
plt.show()

# Plotting boxplots with different colors to show distribution
plt.figure(figsize=(10, 6))
for i, (country, scores) in enumerate(zip(countries, polarity_lists)):
    plt.boxplot(scores, positions=[i], widths=0.6, patch_artist=True,
                boxprops=dict(facecolor=colors[i]), medianprops=dict(color='black'))


plt.xticks(range(len(countries)), countries)
plt.xlabel('Country')
plt.ylabel('TextBlob Polarity Score')
plt.title('Distribution of TextBlob Scores by Country - Polarity')
plt.tick_params(axis='x', labelsize=13)
plt.grid(True)
plt.show()

"""### Subjectivity by country"""

# Calculate subjectivity sentiment for each article
df['Subjectivity'] = df['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

# Group by country and collect all subjectivity scores per country
country_subjectivity = df.groupby('Country')['Subjectivity'].apply(list).reset_index()

# Prepare for plotting
countries = country_subjectivity['Country']
subjectivity_lists = country_subjectivity['Subjectivity']
colors = ['salmon', 'lightblue', 'lightgreen']

# Plotting bar chart for average subjectivity
plt.figure(figsize=(10, 6))
average_subjectivity = [sum(subjectivities) / len(subjectivities) if subjectivities else 0 for subjectivities in subjectivity_lists]
plt.bar(countries, average_subjectivity, color=colors)
plt.xlabel('Country')
plt.ylabel('Average Subjectivity Score')
plt.title('Overall Score by Country - Subjectivity')
plt.xticks(rotation=45)
plt.show()

# Plotting boxplots with different colors to show distribution
plt.figure(figsize=(10, 6))
for i, (country, scores) in enumerate(zip(countries, subjectivity_lists)):
    plt.boxplot(scores, positions=[i], widths=0.6, patch_artist=True,
                boxprops=dict(facecolor=colors[i]), medianprops=dict(color='black'))

plt.xticks(range(len(countries)), countries)
plt.xlabel('Country')
plt.ylabel('TextBlob Subjectivity Score')
plt.title('Distribution of TextBlob Scores by Country - Subjectivity')
plt.tick_params(axis='x', labelsize=13)
plt.grid(True)
plt.show()

from tabulate import tabulate

# Assuming 'df' is your DataFrame and 'Cleaned_Article' contains the text of the articles

# Calculate subjectivity and polarity for each article
df['Subjectivity'] = df['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
df['Polarity'] = df['Cleaned_Article'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Group by country and calculate the range (min, max) for subjectivity and polarity, rounded to two decimals
result_table = df.groupby('Country').agg({
    'Subjectivity': lambda x: (round(min(x), 2), round(max(x), 2)),
    'Polarity': lambda x: (round(min(x), 2), round(max(x), 2))
}).reset_index()

# Rename columns for clarity
result_table.columns = ['Country', 'Subjectivity Range', 'Polarity Range']

# Display the table using tabulate
print(tabulate(result_table, headers='keys', tablefmt='grid'))